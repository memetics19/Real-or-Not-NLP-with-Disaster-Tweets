{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Real or Not? NLP with Disaster Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rrlYGTz3LHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "35efed4e-0900-4d1d-e18c-68cea8475b64"
      },
      "source": [
        "#loading libraries\n",
        "import pandas as pd\n",
        "import string\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import forest \n",
        "from sklearn import tree\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import statistics as stats\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# !pip install scikit-optimize"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdEgc_JP8Jt5",
        "colab_type": "text"
      },
      "source": [
        "# reading the data  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y7ykOzQ4PzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reading the data            \n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "\n",
        "#dummy variables\n",
        "id = df[\"id\"]\n",
        "location = df[\"location\"]\n",
        "keyword = df[\"keyword\"]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb4MUyLS49j8",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK6ds7lheYfG",
        "colab_type": "text"
      },
      "source": [
        "**renaming the text column**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOxkCbh_efcG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "3d359d42-c09e-477b-f861-701a4fac6dcf"
      },
      "source": [
        "df = df.rename(columns={\"text\":\"tweet\"})\n",
        "df[\"original tweet\"] = df[\"tweet\"]\n",
        "print(df)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         id keyword  ... target                                     original tweet\n",
            "0         1     NaN  ...      1  Our Deeds are the Reason of this #earthquake M...\n",
            "1         4     NaN  ...      1             Forest fire near La Ronge Sask. Canada\n",
            "2         5     NaN  ...      1  All residents asked to 'shelter in place' are ...\n",
            "3         6     NaN  ...      1  13,000 people receive #wildfires evacuation or...\n",
            "4         7     NaN  ...      1  Just got sent this photo from Ruby #Alaska as ...\n",
            "...     ...     ...  ...    ...                                                ...\n",
            "7608  10869     NaN  ...      1  Two giant cranes holding a bridge collapse int...\n",
            "7609  10870     NaN  ...      1  @aria_ahrary @TheTawniest The out of control w...\n",
            "7610  10871     NaN  ...      1  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
            "7611  10872     NaN  ...      1  Police investigating after an e-bike collided ...\n",
            "7612  10873     NaN  ...      1  The Latest: More Homes Razed by Northern Calif...\n",
            "\n",
            "[7613 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHF2SAhOulqd",
        "colab_type": "text"
      },
      "source": [
        "***Data Cleaning***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYeOsv_gA5EB",
        "colab_type": "text"
      },
      "source": [
        "**Removing irrelevant features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-_-17QlS8DK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "62a82047-e7b3-4684-f11f-c45b6ac335f1"
      },
      "source": [
        "#removing irrelevant features\n",
        "df = df.drop(columns=[\"id\",\"location\",\"keyword\"],axis=1)\n",
        "print(df)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  tweet  ...                                     original tweet\n",
            "0     Our Deeds are the Reason of this #earthquake M...  ...  Our Deeds are the Reason of this #earthquake M...\n",
            "1                Forest fire near La Ronge Sask. Canada  ...             Forest fire near La Ronge Sask. Canada\n",
            "2     All residents asked to 'shelter in place' are ...  ...  All residents asked to 'shelter in place' are ...\n",
            "3     13,000 people receive #wildfires evacuation or...  ...  13,000 people receive #wildfires evacuation or...\n",
            "4     Just got sent this photo from Ruby #Alaska as ...  ...  Just got sent this photo from Ruby #Alaska as ...\n",
            "...                                                 ...  ...                                                ...\n",
            "7608  Two giant cranes holding a bridge collapse int...  ...  Two giant cranes holding a bridge collapse int...\n",
            "7609  @aria_ahrary @TheTawniest The out of control w...  ...  @aria_ahrary @TheTawniest The out of control w...\n",
            "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...  ...  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
            "7611  Police investigating after an e-bike collided ...  ...  Police investigating after an e-bike collided ...\n",
            "7612  The Latest: More Homes Razed by Northern Calif...  ...  The Latest: More Homes Razed by Northern Calif...\n",
            "\n",
            "[7613 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBYUlMih-D7A",
        "colab_type": "text"
      },
      "source": [
        "**Checking for missing values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HeJtCLc4XpU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "e61d91b7-f751-45d5-daeb-ed2898e257eb"
      },
      "source": [
        "#Dropping variables that has more than 60 percent missing values\n",
        "\n",
        "#Checking the percentage missing values by columns\n",
        "missing_column = (df.isna().sum()/len(df))*100\n",
        "print(missing_column)\n",
        "\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tweet             0.0\n",
            "target            0.0\n",
            "original tweet    0.0\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL9WqeJBVL7F",
        "colab_type": "text"
      },
      "source": [
        "**Checking for duplicate values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suo6PcEA5Us7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "250446c4-98aa-44ac-d180-734621745bb8"
      },
      "source": [
        "duplicate = df.duplicated().sum()\n",
        "print(duplicate)#we can see that there are duplicates values\n",
        " "
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6V6P0L0eCfP",
        "colab_type": "text"
      },
      "source": [
        "**dropping duplicates**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gEYm9mKeL8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop_duplicates()"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4bA3oyh_asA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "834d477e-1364-4c0d-8320-22fae7a0b1ae"
      },
      "source": [
        "sns.countplot(df[\"target\"])\n",
        "#We can see the target column is balanced."
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0500a7f390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMxBxTuuycnK",
        "colab_type": "text"
      },
      "source": [
        "***Cleaning the tweet column***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tfZaSWdnGZk",
        "colab_type": "text"
      },
      "source": [
        "**Removing Hyper Links**\n",
        "\n",
        "By observing the data we can see that some text contains external links (\"http://\"..) which are irrelevant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThsiNP2R70yL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "9dbd4580-80f8-483e-bb59-64fa2149e132"
      },
      "source": [
        "df[\"tweet\"] = df['tweet'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  tweet  ...                                     original tweet\n",
            "0     Our Deeds are the Reason of this #earthquake M...  ...  Our Deeds are the Reason of this #earthquake M...\n",
            "1                Forest fire near La Ronge Sask. Canada  ...             Forest fire near La Ronge Sask. Canada\n",
            "2     All residents asked to 'shelter in place' are ...  ...  All residents asked to 'shelter in place' are ...\n",
            "3     13,000 people receive #wildfires evacuation or...  ...  13,000 people receive #wildfires evacuation or...\n",
            "4     Just got sent this photo from Ruby #Alaska as ...  ...  Just got sent this photo from Ruby #Alaska as ...\n",
            "...                                                 ...  ...                                                ...\n",
            "7604  #WorldNews Fallen powerlines on G:link tram: U...  ...  #WorldNews Fallen powerlines on G:link tram: U...\n",
            "7605  on the flip side I'm at Walmart and there is a...  ...  on the flip side I'm at Walmart and there is a...\n",
            "7606  Suicide bomber kills 15 in Saudi security site...  ...  Suicide bomber kills 15 in Saudi security site...\n",
            "7608  Two giant cranes holding a bridge collapse int...  ...  Two giant cranes holding a bridge collapse int...\n",
            "7612  The Latest: More Homes Razed by Northern Calif...  ...  The Latest: More Homes Razed by Northern Calif...\n",
            "\n",
            "[7521 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-__XHfKXYiV",
        "colab_type": "text"
      },
      "source": [
        "**Removing Punctuation And Changing The Special Characters To The Usual Alphabet Letters** \n",
        "\n",
        "Raw data contain  punctuation,Hyper Link,special character.These value can hamper the performance of model so before applying any text Vectorization first we need to convert raw data into meaningful data which is also called as text preprocessing ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYSm-n9DXsTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "89b14269-55c6-40f3-ff4c-5dd27a795571"
      },
      "source": [
        "#Removing punctuation\n",
        "df[\"tweet\"] = df['tweet'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "\n",
        "\n",
        "#Changing the special characters to the usual alphabet letters\n",
        "df['tweet'] = df[\"tweet\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "print(df[\"tweet\"])\n",
        "\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       Our Deeds are the Reason of this earthquake Ma...\n",
            "1                   Forest fire near La Ronge Sask Canada\n",
            "2       All residents asked to shelter in place are be...\n",
            "3       13000 people receive wildfires evacuation orde...\n",
            "4       Just got sent this photo from Ruby Alaska as s...\n",
            "                              ...                        \n",
            "7604    WorldNews Fallen powerlines on Glink tram UPDA...\n",
            "7605    on the flip side Im at Walmart and there is a ...\n",
            "7606    Suicide bomber kills 15 in Saudi security site...\n",
            "7608    Two giant cranes holding a bridge collapse int...\n",
            "7612    The Latest More Homes Razed by Northern Califo...\n",
            "Name: tweet, Length: 7521, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JCjBYC8__rC",
        "colab_type": "text"
      },
      "source": [
        "**Removing numbers from dataframe**\n",
        "\n",
        "Removing numbers from the text like “1,2,3,4,5…” We will remove numbers because numbers doesn’t give much importance to get the main words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgGq9hOF9tkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['tweet'] = df['tweet'].str.replace('\\d+', '')"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nl49qMx2fBz",
        "colab_type": "text"
      },
      "source": [
        "**Tokenizing data**\n",
        "\n",
        "We use the method word_tokenize() to split a sentence into words. The output of word tokenization can be converted to DataFrame for better text understanding in machine learning applications. It can also be provided as input for further text cleaning steps such as  numeric character removal or stop words removal. Machine learning models need numeric data to be trained and make a prediction. Word tokenization becomes a crucial part of the text (string) to numeric data conversion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp0OoNAt3447",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "c7ecc22b-921b-45af-aea6-2555c1232671"
      },
      "source": [
        "df[\"tweet\"] = [word_tokenize(word) for word in df[\"tweet\"]]\n",
        "print(df[\"tweet\"])\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       [Our, Deeds, are, the, Reason, of, this, earth...\n",
            "1           [Forest, fire, near, La, Ronge, Sask, Canada]\n",
            "2       [All, residents, asked, to, shelter, in, place...\n",
            "3       [people, receive, wildfires, evacuation, order...\n",
            "4       [Just, got, sent, this, photo, from, Ruby, Ala...\n",
            "                              ...                        \n",
            "7604    [WorldNews, Fallen, powerlines, on, Glink, tra...\n",
            "7605    [on, the, flip, side, Im, at, Walmart, and, th...\n",
            "7606    [Suicide, bomber, kills, in, Saudi, security, ...\n",
            "7608    [Two, giant, cranes, holding, a, bridge, colla...\n",
            "7612    [The, Latest, More, Homes, Razed, by, Northern...\n",
            "Name: tweet, Length: 7521, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3RqSad4um3Q",
        "colab_type": "text"
      },
      "source": [
        "**Conveting text to Lower Case**\n",
        "\n",
        "The model might treat a word which is in the beginning of a sentence with a capital letter different from the same word which appears later in the sentence but without any capital latter. This might lead to decline in the accuracy. Whereas lowering the words would be a better trade off.So that 'A' letter differ from 'a' letter in computer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBVW72bhu16O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "0b0d0c0d-0408-46aa-eba9-eb180398be1f"
      },
      "source": [
        "df[\"tweet\"] = [[word.lower() for word in words ] for words in df[\"tweet\"]]\n",
        "print(df)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  tweet  ...                                     original tweet\n",
            "0     [our, deeds, are, the, reason, of, this, earth...  ...  Our Deeds are the Reason of this #earthquake M...\n",
            "1         [forest, fire, near, la, ronge, sask, canada]  ...             Forest fire near La Ronge Sask. Canada\n",
            "2     [all, residents, asked, to, shelter, in, place...  ...  All residents asked to 'shelter in place' are ...\n",
            "3     [people, receive, wildfires, evacuation, order...  ...  13,000 people receive #wildfires evacuation or...\n",
            "4     [just, got, sent, this, photo, from, ruby, ala...  ...  Just got sent this photo from Ruby #Alaska as ...\n",
            "...                                                 ...  ...                                                ...\n",
            "7604  [worldnews, fallen, powerlines, on, glink, tra...  ...  #WorldNews Fallen powerlines on G:link tram: U...\n",
            "7605  [on, the, flip, side, im, at, walmart, and, th...  ...  on the flip side I'm at Walmart and there is a...\n",
            "7606  [suicide, bomber, kills, in, saudi, security, ...  ...  Suicide bomber kills 15 in Saudi security site...\n",
            "7608  [two, giant, cranes, holding, a, bridge, colla...  ...  Two giant cranes holding a bridge collapse int...\n",
            "7612  [the, latest, more, homes, razed, by, northern...  ...  The Latest: More Homes Razed by Northern Calif...\n",
            "\n",
            "[7521 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9uAApl7gm5i",
        "colab_type": "text"
      },
      "source": [
        "**Removing stop words from texts**\n",
        "\n",
        "Removing stopwords can potentially help improve the performance as \n",
        "there are fewer and only meaningful tokens left.\n",
        "Thus, it could increase classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvPzLs2J_SKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "58937110-7142-4f7c-b2ad-ad72f31f226d"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "\n",
        "df[\"tweet\"] = [[words for words in word if words not in stop_words] for word in df[\"tweet\"]]\n",
        "print(df[\"tweet\"])\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       [deeds, reason, earthquake, may, allah, forgiv...\n",
            "1           [forest, fire, near, la, ronge, sask, canada]\n",
            "2       [residents, asked, shelter, place, notified, o...\n",
            "3       [people, receive, wildfires, evacuation, order...\n",
            "4       [got, sent, photo, ruby, alaska, smoke, wildfi...\n",
            "                              ...                        \n",
            "7604    [worldnews, fallen, powerlines, glink, tram, u...\n",
            "7605    [flip, side, im, walmart, bomb, everyone, evac...\n",
            "7606    [suicide, bomber, kills, saudi, security, site...\n",
            "7608    [two, giant, cranes, holding, bridge, collapse...\n",
            "7612    [latest, homes, razed, northern, california, w...\n",
            "Name: tweet, Length: 7521, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPpXW-nsOaAk",
        "colab_type": "text"
      },
      "source": [
        "**Lematization**\n",
        "\n",
        "Lemmatization usually aims to remove word endings. It helps in returning the base or dictionary form of a word, which is known as the lemma.\n",
        "\n",
        "<!-- congrats yey you found the easter egg (hahaha) \n",
        "\n",
        "text me to claim your reward -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kzOaROMXq3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lem = nltk.WordNetLemmatizer()\n",
        "df[\"tweet\"] = [[lem.lemmatize(lema,\"v\") for lema in i]for i in df[\"tweet\"]]\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4wrytX4e8or",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GLRQsQn8kLd",
        "colab_type": "text"
      },
      "source": [
        "**Declaring Inpendent and Dependent Variable**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRw2Jbm1DEH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = df[\"tweet\"]\n",
        "y = df[\"target\"]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP_UbJhmg2RF",
        "colab_type": "text"
      },
      "source": [
        "**Splitting the data**\n",
        "\n",
        "Now the data is clean we will be Spltting the dataframe into training and testing sample of 80% and 20% respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWYfgTiKGJeV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1f8d0e26-3cd9-417b-801f-8c18d611a506"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split = train_test_split(x,y, test_size=0.2, random_state=0)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6016,)\n",
            "(1505,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3LIVzfu-JwI",
        "colab_type": "text"
      },
      "source": [
        "**Converting text to numeric**\n",
        "\n",
        "We cannot work with text directly when using machine learning algorithms. Instead, we need to convert the text to numbers.\n",
        "Computers don’t understand text and only understand and process numbers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyNgEMZCq-X4",
        "colab_type": "text"
      },
      "source": [
        "When applying TfidfVectorizer,CountVectorization etc on text they expect an array of string that has not been tokenized. So if you pass him an array of arrays of tokenz, it crashes.We will  be passing a tokenized text to the vectorizer, to deal with this We need to pass a dummy fuction to  tokenizer and preprocessor parameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAVkRA50pyxH",
        "colab_type": "text"
      },
      "source": [
        "Count vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whTtcBzg-1cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Creating a dummy fuction so it can be passed to the  (tokenizer and preprocessor) parameter\n",
        "# def dummy(doc):\n",
        "#     return doc\n",
        "\n",
        "# cv = CountVectorizer(\n",
        "#         tokenizer=dummy,\n",
        "#         preprocessor=dummy,\n",
        "#         min_df = 0.000167\n",
        "        \n",
        "#     )  \n",
        "\n",
        "# x_train = cv.fit_transform(x_train)\n",
        "\n",
        "# x_test =  cv.transform(x_test)\n",
        "\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dil-mpu-JnF",
        "colab_type": "text"
      },
      "source": [
        "Tfidfvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR83lelkp3T3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dummy(doc):\n",
        "    return doc\n",
        "\n",
        "tfidf = TfidfVectorizer(#stop_words='english',\n",
        "         tokenizer=dummy,\n",
        "         preprocessor=dummy,\n",
        "         min_df = 0.000167\n",
        "\n",
        ")\n",
        "\n",
        "x_train = tfidf.fit_transform(x_train)\n",
        "\n",
        "x_test =  tfidf.transform(x_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtvJ1U2DVjcF",
        "colab_type": "text"
      },
      "source": [
        "Using Cross Validation to find the algorithm that gives the best performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj2hWouyVlV5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "5954037f-fa46-4ba3-ed7f-5ce575b64c6d"
      },
      "source": [
        "xg = xgb.XGBClassifier()\n",
        "fo =  forest.RandomForestClassifier()\n",
        "tr = tree.DecisionTreeClassifier()\n",
        "lo = linear_model.LogisticRegression()\n",
        "sv = svm.SVC()\n",
        "\n",
        "xgb_score = cross_val_score(xg,x_train,y_train,cv=5)\n",
        "ran_score = cross_val_score(fo,x_train,y_train,cv=5)\n",
        "dtree_score = cross_val_score(tr,x_train,y_train,cv=5)\n",
        "log_score = cross_val_score(lo,x_train,y_train,cv=5)\n",
        "svm_score = cross_val_score(sv,x_train,y_train,cv=5)\n",
        "\n",
        "# This Dataframe outputs the average score for each algorithms\n",
        "df_score = pd.DataFrame({\"model\":[\"xgboost\",\"RandomForestClassifier\",\"DecisionTreeClassifier\",\"LogisticRegression\",\"Support vector machine\"],\"score\":[stats.mean(xgb_score),stats.mean(ran_score),stats.mean(dtree_score),stats.mean(log_score),stats.mean(svm_score)]})\n",
        "print(df_score)\n",
        "# We can see that Support vector machine classifier gave the best score\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    model     score\n",
            "0                 xgboost  0.734041\n",
            "1  RandomForestClassifier  0.785737\n",
            "2  DecisionTreeClassifier  0.730050\n",
            "3      LogisticRegression  0.797705\n",
            "4  Support vector machine  0.799368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUNg4QhlljKl",
        "colab_type": "text"
      },
      "source": [
        "**Hyperparameter Tuning Using Using BayesSearchCV**\n",
        "\n",
        "Finding the hyperparameter values of a learning algorithm that produces the best result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuaVUfBpphXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "142020db-84b8-450e-8af6-7d72072dd452"
      },
      "source": [
        "#Checking initial model score\n",
        "initial_model = svm.SVC()\n",
        "initial_model = initial_model.fit(x_train,y_train)\n",
        "original_score = initial_model.score(x_test,y_test)\n",
        "print(f'Original Score = {original_score}')\n",
        "\n",
        "# Count vectorization score = Score = 0.7906976744186046\n",
        "# Tfidf score = Score = 0.7933554817275748\n",
        "\n",
        "#We see Tfidf gives the best score..so tfidf will be used for vectorization\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Score = 0.7933554817275748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur6MJK3s8S4k",
        "colab_type": "text"
      },
      "source": [
        "Optimizing parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfNMWTxgmdo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finding the best parameter\n",
        "# optimize_model  = svm.SVC()\n",
        "# param = {'C': [0.1,1, 10, 52,100], 'gamma': ('auto','scale'),'kernel': ['linear','rbf', 'poly', 'sigmoid']}\n",
        "# search = BayesSearchCV(optimize_model,param,scoring=\"accuracy\")\n",
        "# search = search.fit(x_train,y_train)\n",
        "# print(search.best_params_)\n",
        "\n",
        "#best_param = C=1.0,gamma='scale',kernel='rbf'\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q70S9KDVRpJt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Optimized model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuFQ7gQERmzW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "360d937e-41aa-49cf-c5a8-0ce19c58da89"
      },
      "source": [
        "model = svm.SVC(C=1.0,gamma=1.0,kernel='rbf')\n",
        "model = model.fit(x_train,y_train)\n",
        "score = model.score(x_test,y_test)\n",
        "print(f'model Score = {score}')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model Score = 0.7933554817275748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXVCWLT0o4Jz",
        "colab_type": "text"
      },
      "source": [
        "# Predicting which Tweets are about real disasters and which ones are not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yZYYoTZuWfJ",
        "colab_type": "text"
      },
      "source": [
        "**Reading the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFVrmwRbTU-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "140853ae-e7bd-4087-8a47-b445c58aa816"
      },
      "source": [
        "test = pd.read_csv(\"clean_test.csv\")\n",
        "#dummy variables\n",
        "tweet_id =  test[\"id\"]\n",
        "tweets = test[\"tweet\"]\n",
        "print(tweets.shape)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3263,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3mVsNO9MaM9",
        "colab_type": "text"
      },
      "source": [
        "**converting tweets to numeric using Tfidf vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGeBeKhEMHnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = tfidf.transform(tweets)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCSMOupxTLM_",
        "colab_type": "text"
      },
      "source": [
        "**Predicting test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF2aMtsoSiqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "9b2ef912-96f9-4462-ca4c-88d11c89fd5d"
      },
      "source": [
        "pred = model.predict(tweets)\n",
        "new_df = {\"id\":tweet_id,\"target\":pred}\n",
        "new_df = pd.DataFrame(new_df)\n",
        "print(new_df)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         id  target\n",
            "0         0       0\n",
            "1         2       1\n",
            "2         3       0\n",
            "3         9       0\n",
            "4        11       0\n",
            "...     ...     ...\n",
            "3258  10861       0\n",
            "3259  10865       0\n",
            "3260  10868       1\n",
            "3261  10874       0\n",
            "3262  10875       0\n",
            "\n",
            "[3263 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pi3Xma_csxX",
        "colab_type": "text"
      },
      "source": [
        "saving to csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ply38GKPLAxD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "8030c651-2529-4bda-b6d2-085a74982370"
      },
      "source": [
        "disaster_pred = new_df.to_csv(\"disaster_pred.csv\",index = False)\n",
        "print(disaster_pred)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl2z2GhjwoUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dcHrGmVuY3y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOCUKvf9AV_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7vYjQkYkNMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoRA_g-NowIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 108,
      "outputs": []
    }
  ]
}