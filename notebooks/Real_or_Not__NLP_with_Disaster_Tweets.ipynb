{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Real or Not? NLP with Disaster Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rrlYGTz3LHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading libraries\n",
        "import pandas as pd\n",
        "import string\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import forest \n",
        "from sklearn import tree\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import statistics as stats\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# !pip install scikit-optimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdEgc_JP8Jt5",
        "colab_type": "text"
      },
      "source": [
        "# reading the data  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y7ykOzQ4PzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reading the data            \n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "\n",
        "#dummy variables\n",
        "id = df[\"id\"]\n",
        "location = df[\"location\"]\n",
        "keyword = df[\"keyword\"]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb4MUyLS49j8",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK6ds7lheYfG",
        "colab_type": "text"
      },
      "source": [
        "**renaming the text column**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOxkCbh_efcG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "55f56717-cbfe-4361-f94d-310231029b66"
      },
      "source": [
        "df = df.rename(columns={\"text\":\"tweet\"})\n",
        "df[\"original tweet\"] = df[\"tweet\"]\n",
        "df.head()"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>tweet</th>\n",
              "      <th>target</th>\n",
              "      <th>original tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ... target                                     original tweet\n",
              "0   1     NaN  ...      1  Our Deeds are the Reason of this #earthquake M...\n",
              "1   4     NaN  ...      1             Forest fire near La Ronge Sask. Canada\n",
              "2   5     NaN  ...      1  All residents asked to 'shelter in place' are ...\n",
              "3   6     NaN  ...      1  13,000 people receive #wildfires evacuation or...\n",
              "4   7     NaN  ...      1  Just got sent this photo from Ruby #Alaska as ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHF2SAhOulqd",
        "colab_type": "text"
      },
      "source": [
        "***Data Cleaning***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYeOsv_gA5EB",
        "colab_type": "text"
      },
      "source": [
        "**Removing irrelevant features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-_-17QlS8DK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "9762a848-9832-4215-94c9-a16f0cdac482"
      },
      "source": [
        "#removing irrelevant features\n",
        "df = df.drop(columns=[\"id\",\"location\",\"keyword\"],axis=1)\n",
        "df.head()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>target</th>\n",
              "      <th>original tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  ...                                     original tweet\n",
              "0  Our Deeds are the Reason of this #earthquake M...  ...  Our Deeds are the Reason of this #earthquake M...\n",
              "1             Forest fire near La Ronge Sask. Canada  ...             Forest fire near La Ronge Sask. Canada\n",
              "2  All residents asked to 'shelter in place' are ...  ...  All residents asked to 'shelter in place' are ...\n",
              "3  13,000 people receive #wildfires evacuation or...  ...  13,000 people receive #wildfires evacuation or...\n",
              "4  Just got sent this photo from Ruby #Alaska as ...  ...  Just got sent this photo from Ruby #Alaska as ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBYUlMih-D7A",
        "colab_type": "text"
      },
      "source": [
        "**Checking for missing values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HeJtCLc4XpU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "00ab938d-e58b-419f-8c01-f8807d6d3b11"
      },
      "source": [
        "#Dropping variables that has more than 60 percent missing values\n",
        "\n",
        "#Checking the percentage missing values by columns\n",
        "missing_column = (df.isna().sum()/len(df))*100\n",
        "print(missing_column)\n",
        "\n"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tweet             0.0\n",
            "target            0.0\n",
            "original tweet    0.0\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL9WqeJBVL7F",
        "colab_type": "text"
      },
      "source": [
        "**Checking for duplicate values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suo6PcEA5Us7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75c1f473-a826-426d-de9f-e58458e9d5ec"
      },
      "source": [
        "duplicate = df.duplicated().sum()\n",
        "print(duplicate)#we can see that there are duplicates values\n",
        " "
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6V6P0L0eCfP",
        "colab_type": "text"
      },
      "source": [
        "**dropping duplicates**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gEYm9mKeL8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop_duplicates()"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4bA3oyh_asA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "2b425374-1029-4bc3-b6cb-94020c8db50d"
      },
      "source": [
        "sns.countplot(df[\"target\"])\n",
        "#We can see the target column is balanced."
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f45c40a30b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMxBxTuuycnK",
        "colab_type": "text"
      },
      "source": [
        "***Cleaning the tweet column***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tfZaSWdnGZk",
        "colab_type": "text"
      },
      "source": [
        "**Removing Hyper Links**\n",
        "\n",
        "By observing the data we can see that some text contains external links (\"http://\"..) which are irrelevant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThsiNP2R70yL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "0be2afca-cce3-4ddd-b52f-9a5f6e13cbed"
      },
      "source": [
        "df[\"tweet\"] = df['tweet'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>target</th>\n",
              "      <th>original tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  ...                                     original tweet\n",
              "0  Our Deeds are the Reason of this #earthquake M...  ...  Our Deeds are the Reason of this #earthquake M...\n",
              "1             Forest fire near La Ronge Sask. Canada  ...             Forest fire near La Ronge Sask. Canada\n",
              "2  All residents asked to 'shelter in place' are ...  ...  All residents asked to 'shelter in place' are ...\n",
              "3  13,000 people receive #wildfires evacuation or...  ...  13,000 people receive #wildfires evacuation or...\n",
              "4  Just got sent this photo from Ruby #Alaska as ...  ...  Just got sent this photo from Ruby #Alaska as ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-__XHfKXYiV",
        "colab_type": "text"
      },
      "source": [
        "**Removing Punctuation And Changing The Special Characters To The Usual Alphabet Letters** \n",
        "\n",
        "Raw data contain  punctuation,Hyper Link,special character.These value can hamper the performance of model so before applying any text Vectorization first we need to convert raw data into meaningful data which is also called as text preprocessing ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYSm-n9DXsTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "4b7cf189-153e-4f41-d7e0-6a95a47aacbd"
      },
      "source": [
        "#Removing punctuation\n",
        "df[\"tweet\"] = df['tweet'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "\n",
        "\n",
        "#Changing the special characters to the usual alphabet letters\n",
        "df['tweet'] = df[\"tweet\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "df[\"tweet\"].head()\n",
        "\n"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Our Deeds are the Reason of this earthquake Ma...\n",
              "1                Forest fire near La Ronge Sask Canada\n",
              "2    All residents asked to shelter in place are be...\n",
              "3    13000 people receive wildfires evacuation orde...\n",
              "4    Just got sent this photo from Ruby Alaska as s...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JCjBYC8__rC",
        "colab_type": "text"
      },
      "source": [
        "**Removing numbers from dataframe**\n",
        "\n",
        "Removing numbers from the text like “1,2,3,4,5…” We will remove numbers because numbers doesn’t give much importance to get the main words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgGq9hOF9tkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['tweet'] = df['tweet'].str.replace('\\d+', '')"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nl49qMx2fBz",
        "colab_type": "text"
      },
      "source": [
        "**Tokenizing data**\n",
        "\n",
        "We use the method word_tokenize() to split a sentence into words. The output of word tokenization can be converted to DataFrame for better text understanding in machine learning applications. It can also be provided as input for further text cleaning steps such as  numeric character removal or stop words removal. Machine learning models need numeric data to be trained and make a prediction. Word tokenization becomes a crucial part of the text (string) to numeric data conversion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp0OoNAt3447",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "79e028a1-013d-4149-f91e-2075ac5208c7"
      },
      "source": [
        "df[\"tweet\"] = [word_tokenize(word) for word in df[\"tweet\"]]\n",
        "df.head()\n"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>target</th>\n",
              "      <th>original tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[All, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
              "      <td>1</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  ...                                     original tweet\n",
              "0  [Our, Deeds, are, the, Reason, of, this, earth...  ...  Our Deeds are the Reason of this #earthquake M...\n",
              "1      [Forest, fire, near, La, Ronge, Sask, Canada]  ...             Forest fire near La Ronge Sask. Canada\n",
              "2  [All, residents, asked, to, shelter, in, place...  ...  All residents asked to 'shelter in place' are ...\n",
              "3  [people, receive, wildfires, evacuation, order...  ...  13,000 people receive #wildfires evacuation or...\n",
              "4  [Just, got, sent, this, photo, from, Ruby, Ala...  ...  Just got sent this photo from Ruby #Alaska as ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3RqSad4um3Q",
        "colab_type": "text"
      },
      "source": [
        "**Conveting text to Lower Case**\n",
        "\n",
        "The model might treat a word which is in the beginning of a sentence with a capital letter different from the same word which appears later in the sentence but without any capital latter. This might lead to decline in the accuracy. Whereas lowering the words would be a better trade off.So that 'A' letter differ from 'a' letter in computer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBVW72bhu16O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "bdf23237-ee0c-4934-cae7-c0c45092220f"
      },
      "source": [
        "df[\"tweet\"] = [[word.lower() for word in words ] for words in df[\"tweet\"]]\n",
        "df.head()"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>target</th>\n",
              "      <th>original tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
              "      <td>1</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  ...                                     original tweet\n",
              "0  [our, deeds, are, the, reason, of, this, earth...  ...  Our Deeds are the Reason of this #earthquake M...\n",
              "1      [forest, fire, near, la, ronge, sask, canada]  ...             Forest fire near La Ronge Sask. Canada\n",
              "2  [all, residents, asked, to, shelter, in, place...  ...  All residents asked to 'shelter in place' are ...\n",
              "3  [people, receive, wildfires, evacuation, order...  ...  13,000 people receive #wildfires evacuation or...\n",
              "4  [just, got, sent, this, photo, from, ruby, ala...  ...  Just got sent this photo from Ruby #Alaska as ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9uAApl7gm5i",
        "colab_type": "text"
      },
      "source": [
        "**Removing stop words from texts**\n",
        "\n",
        "Removing stopwords can potentially help improve the performance as \n",
        "there are fewer and only meaningful tokens left.\n",
        "Thus, it could increase classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvPzLs2J_SKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "0e8519ee-520f-461e-a4b9-d6137da0e40a"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "\n",
        "df[\"tweet\"] = [[words for words in word if words not in stop_words] for word in df[\"tweet\"]]\n",
        "df.head()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>target</th>\n",
              "      <th>original tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
              "      <td>1</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  ...                                     original tweet\n",
              "0  [deeds, reason, earthquake, may, allah, forgiv...  ...  Our Deeds are the Reason of this #earthquake M...\n",
              "1      [forest, fire, near, la, ronge, sask, canada]  ...             Forest fire near La Ronge Sask. Canada\n",
              "2  [residents, asked, shelter, place, notified, o...  ...  All residents asked to 'shelter in place' are ...\n",
              "3  [people, receive, wildfires, evacuation, order...  ...  13,000 people receive #wildfires evacuation or...\n",
              "4  [got, sent, photo, ruby, alaska, smoke, wildfi...  ...  Just got sent this photo from Ruby #Alaska as ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPpXW-nsOaAk",
        "colab_type": "text"
      },
      "source": [
        "**Lematization**\n",
        "\n",
        "Lemmatization usually aims to remove word endings. It helps in returning the base or dictionary form of a word, which is known as the lemma.\n",
        "\n",
        "<!-- congrats yey you found the easter egg (hahaha) \n",
        "\n",
        "text me to claim your reward -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kzOaROMXq3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lem = nltk.WordNetLemmatizer()\n",
        "df[\"tweet\"] = [[lem.lemmatize(lema,\"v\") for lema in i]for i in df[\"tweet\"]]\n"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4wrytX4e8or",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GLRQsQn8kLd",
        "colab_type": "text"
      },
      "source": [
        "**Declaring Inpendent and Dependent Variable**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRw2Jbm1DEH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = df[\"tweet\"]\n",
        "y = df[\"target\"]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP_UbJhmg2RF",
        "colab_type": "text"
      },
      "source": [
        "**Splitting the data**\n",
        "\n",
        "Now the data is clean we will be Spltting the dataframe into training and testing sample of 80% and 20% respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWYfgTiKGJeV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "13bb27b7-ba5c-4a5c-8565-0334552e5b76"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split = train_test_split(x,y, test_size=0.2, random_state=0)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6016,)\n",
            "(1505,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3LIVzfu-JwI",
        "colab_type": "text"
      },
      "source": [
        "**Converting text to numeric**\n",
        "\n",
        "We cannot work with text directly when using machine learning algorithms. Instead, we need to convert the text to numbers.\n",
        "Computers don’t understand text and only understand and process numbers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyNgEMZCq-X4",
        "colab_type": "text"
      },
      "source": [
        "When applying TfidfVectorizer,CountVectorization etc on text they expect an array of string that has not been tokenized. So if you pass him an array of arrays of tokenz, it crashes.We will  be passing a tokenized text to the vectorizer, to deal with this We need to pass a dummy fuction to  tokenizer and preprocessor parameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAVkRA50pyxH",
        "colab_type": "text"
      },
      "source": [
        "Count vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whTtcBzg-1cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Creating a dummy fuction so it can be passed to the  (tokenizer and preprocessor) parameter\n",
        "# def dummy(doc):\n",
        "#     return doc\n",
        "\n",
        "# cv = CountVectorizer(\n",
        "#         tokenizer=dummy,\n",
        "#         preprocessor=dummy,\n",
        "#         min_df = 0.000167\n",
        "        \n",
        "#     )  \n",
        "\n",
        "# x_train = cv.fit_transform(x_train)\n",
        "\n",
        "# x_test =  cv.transform(x_test)\n",
        "\n"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dil-mpu-JnF",
        "colab_type": "text"
      },
      "source": [
        "Tfidfvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR83lelkp3T3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dummy(doc):\n",
        "    return doc\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "         tokenizer=dummy,\n",
        "         preprocessor=dummy,\n",
        "         min_df = 0.000167\n",
        "\n",
        ")\n",
        "\n",
        "x_train = tfidf.fit_transform(x_train)\n",
        "\n",
        "x_test =  tfidf.transform(x_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtvJ1U2DVjcF",
        "colab_type": "text"
      },
      "source": [
        "Using Cross Validation to find the algorithm that gives the best performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj2hWouyVlV5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "a7f89d5c-cc49-4bf4-9c88-a33601875260"
      },
      "source": [
        "xg = xgb.XGBClassifier()\n",
        "fo =  forest.RandomForestClassifier()\n",
        "tr = tree.DecisionTreeClassifier()\n",
        "lo = linear_model.LogisticRegression()\n",
        "sv = svm.SVC()\n",
        "\n",
        "xgb_score = cross_val_score(xg,x_train,y_train,cv=5)\n",
        "ran_score = cross_val_score(fo,x_train,y_train,cv=5)\n",
        "dtree_score = cross_val_score(tr,x_train,y_train,cv=5)\n",
        "log_score = cross_val_score(lo,x_train,y_train,cv=5)\n",
        "svm_score = cross_val_score(sv,x_train,y_train,cv=5)\n",
        "\n",
        "# This Dataframe outputs the average score for each algorithms\n",
        "df_score = pd.DataFrame({\"model\":[\"xgboost\",\"RandomForestClassifier\",\"DecisionTreeClassifier\",\"LogisticRegression\",\"Support vector machine\"],\"score\":[stats.mean(xgb_score),stats.mean(ran_score),stats.mean(dtree_score),stats.mean(log_score),stats.mean(svm_score)]})\n",
        "df_score\n",
        "# We can see that Support vector machine classifier gave the best score\n"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xgboost</td>\n",
              "      <td>0.734041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>0.783578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DecisionTreeClassifier</td>\n",
              "      <td>0.727057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.797705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Support vector machine</td>\n",
              "      <td>0.799368</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    model     score\n",
              "0                 xgboost  0.734041\n",
              "1  RandomForestClassifier  0.783578\n",
              "2  DecisionTreeClassifier  0.727057\n",
              "3      LogisticRegression  0.797705\n",
              "4  Support vector machine  0.799368"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUNg4QhlljKl",
        "colab_type": "text"
      },
      "source": [
        "**Hyperparameter Tuning Using Using BayesSearchCV**\n",
        "\n",
        "Finding the hyperparameter values of a learning algorithm that produces the best result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuaVUfBpphXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a878c31-7781-44ab-ce11-baf2bbcabb2e"
      },
      "source": [
        "#Checking initial model score\n",
        "initial_model = svm.SVC()\n",
        "initial_model = initial_model.fit(x_train,y_train)\n",
        "original_score = initial_model.score(x_test,y_test)\n",
        "print(f'Original Score = {original_score}')\n",
        "\n",
        "# Count vectorization score = Score = 0.7906976744186046\n",
        "# Tfidf score = Score = 0.7933554817275748\n",
        "\n",
        "#We see Tfidf gives the best score..so tfidf will be used for vectorization\n"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Score = 0.7933554817275748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur6MJK3s8S4k",
        "colab_type": "text"
      },
      "source": [
        "Optimizing parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfNMWTxgmdo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finding the best parameter\n",
        "# optimize_model  = svm.SVC()\n",
        "# param = {'C': [0.1,1, 10, 52,100], 'gamma': ('auto','scale'),'kernel': ['linear','rbf', 'poly', 'sigmoid']}\n",
        "# search = BayesSearchCV(optimize_model,param,scoring=\"accuracy\")\n",
        "# search = search.fit(x_train,y_train)\n",
        "# print(search.best_params_)\n",
        "\n",
        "#best_param = C=1.0,gamma='scale',kernel='rbf'\n"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q70S9KDVRpJt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Optimized model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuFQ7gQERmzW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29dd63fb-32ec-4f6d-dc02-41937014ddd5"
      },
      "source": [
        "model = svm.SVC(C=1.0,gamma=1.0,kernel='rbf')\n",
        "model = model.fit(x_train,y_train)\n",
        "score = model.score(x_test,y_test)\n",
        "print(f'model Score = {score}')"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model Score = 0.7933554817275748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXVCWLT0o4Jz",
        "colab_type": "text"
      },
      "source": [
        "# Predicting which Tweets are about real disasters and which ones are not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yZYYoTZuWfJ",
        "colab_type": "text"
      },
      "source": [
        "**Reading the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFVrmwRbTU-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53478a96-eec2-4ea0-d5d5-eec428a4a1c5"
      },
      "source": [
        "test = pd.read_csv(\"clean_test.csv\")\n",
        "#dummy variables\n",
        "tweet_id =  test[\"id\"]\n",
        "tweets = test[\"tweet\"]\n",
        "print(tweets.shape)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3263,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3mVsNO9MaM9",
        "colab_type": "text"
      },
      "source": [
        "**converting tweets to numeric using Tfidf vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGeBeKhEMHnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = tfidf.transform(tweets)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCSMOupxTLM_",
        "colab_type": "text"
      },
      "source": [
        "**Predicting test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF2aMtsoSiqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "4b48251b-69fc-4690-eb7f-ca4dce91f82b"
      },
      "source": [
        "pred = model.predict(tweets)\n",
        "new_df = {\"id\":tweet_id,\"target\":pred}\n",
        "new_df = pd.DataFrame(new_df)\n",
        "new_df.head()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  target\n",
              "0   0       0\n",
              "1   2       1\n",
              "2   3       0\n",
              "3   9       0\n",
              "4  11       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pi3Xma_csxX",
        "colab_type": "text"
      },
      "source": [
        "saving to csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ply38GKPLAxD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36e8aaae-7f68-48e9-8ceb-527036a417ef"
      },
      "source": [
        "disaster_pred = new_df.to_csv(\"disaster_pred.csv\",index = False)\n",
        "print(disaster_pred)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl2z2GhjwoUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dcHrGmVuY3y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOCUKvf9AV_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7vYjQkYkNMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoRA_g-NowIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 166,
      "outputs": []
    }
  ]
}